{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b795246",
   "metadata": {},
   "source": [
    "## Seis nuevas variables:\n",
    "\n",
    "* **user_count:** número de calificaciones realizadas por cada usuario.\n",
    "\n",
    "* **movie_count:** número de calificaciones recibidas por cada película.\n",
    "\n",
    "* **avg_rating_by_user:** promedio de las calificaciones dadas por cada usuario.\n",
    "\n",
    "* **avg_rating_for_movie:** promedio de las calificaciones recibidas por cada película.\n",
    "\n",
    "* **freq_pair_count:** cantidad de pares frecuentes (con soporte ≥ 20 %) que incluyen la película en la transacción del usuario.\n",
    "\n",
    "* **freq_pair_support_sum:** suma de los soportes de esos pares frecuentes para la película y el usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a94e67",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE baseline (media global=3.6912): 1.7342\n",
      "MEJORA relativa sobre baseline: 9.3%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carga y limpieza igual que antes  \n",
    "df = pd.read_csv('ratings2comoML.csv').drop('timestamp', axis=1)\n",
    "\n",
    "# 1) Calcula la media global y su RMSE (baseline)\n",
    "global_mean = df['rating'].mean()\n",
    "y_true = df['rating']\n",
    "y_pred_baseline = np.full_like(y_true, global_mean, dtype=float)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_true, y_pred_baseline))\n",
    "\n",
    "print(f'RMSE baseline (media global={global_mean:.4f}): {baseline_rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66575e63",
   "metadata": {},
   "source": [
    "---\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34caeb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4116\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Usuarios por película\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "total_users = df['userId'].nunique()\n",
    "min_support = 0.2  # umbral de soporte mínimo\n",
    "\n",
    "# 3. Generación de ítems frecuentes de tamaño 2 y 3\n",
    "frequent_pairs = {}\n",
    "frequent_triples = {}\n",
    "\n",
    "# Ítems frecuentes de tamaño 2\n",
    "for m1, m2 in combinations(users_by_movie.keys(), 2):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_pairs[frozenset({m1, m2})] = support\n",
    "\n",
    "# Ítems frecuentes de tamaño 3\n",
    "for m1, m2, m3 in combinations(users_by_movie.keys(), 3):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_triples[frozenset({m1, m2, m3})] = support\n",
    "\n",
    "# 4. Función para extraer características basadas en Apriori\n",
    "def apriori_features(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "\n",
    "    pair_count = 0\n",
    "    pair_sum = 0.0\n",
    "    triple_count = 0\n",
    "    triple_sum = 0.0\n",
    "\n",
    "    # Pares frecuentes\n",
    "    for other in rated:\n",
    "        pair = frozenset({target, other})\n",
    "        if pair in frequent_pairs:\n",
    "            pair_count += 1\n",
    "            pair_sum += frequent_pairs[pair]\n",
    "\n",
    "    # Tríos frecuentes\n",
    "    for combo in combinations(rated, 2):\n",
    "        triple = frozenset((target,) + combo)\n",
    "        if triple in frequent_triples:\n",
    "            triple_count += 1\n",
    "            triple_sum += frequent_triples[triple]\n",
    "\n",
    "    return pd.Series({\n",
    "        'freq_pair_count': pair_count,\n",
    "        'freq_pair_support_sum': pair_sum,\n",
    "        'freq_triple_count': triple_count,\n",
    "        'freq_triple_support_sum': triple_sum\n",
    "    })\n",
    "\n",
    "# 5. Aplicar características y preparar dataset final\n",
    "apriori_feats = df.apply(apriori_features, axis=1)\n",
    "df_feat = pd.concat([df, apriori_feats], axis=1)\n",
    "df_final = df_feat.drop(['userId', 'movieId', 'timestamp'], axis=1)\n",
    "\n",
    "X = df_final.drop('rating', axis=1)\n",
    "y = df_final['rating']\n",
    "\n",
    "# 6. Entrenar modelo y calcular RMSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f9f523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>freq_pair_count</th>\n",
       "      <th>freq_pair_support_sum</th>\n",
       "      <th>freq_triple_count</th>\n",
       "      <th>freq_triple_support_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    rating  freq_pair_count  freq_pair_support_sum  freq_triple_count  \\\n",
       "0        5              5.0                    1.7               10.0   \n",
       "1        5              6.0                    2.4               11.0   \n",
       "2        5              5.0                    1.5               10.0   \n",
       "3        5              6.0                    1.9               10.0   \n",
       "4        5              5.0                    1.9               10.0   \n",
       "..     ...              ...                    ...                ...   \n",
       "63       5              5.0                    2.3               10.0   \n",
       "64       5              5.0                    2.5               10.0   \n",
       "65       5              5.0                    2.0               10.0   \n",
       "66       1              5.0                    2.3               10.0   \n",
       "67       5              5.0                    2.3               10.0   \n",
       "\n",
       "    freq_triple_support_sum  \n",
       "0                       2.9  \n",
       "1                       3.2  \n",
       "2                       2.6  \n",
       "3                       2.9  \n",
       "4                       3.0  \n",
       "..                      ...  \n",
       "63                      4.2  \n",
       "64                      4.2  \n",
       "65                      3.6  \n",
       "66                      4.2  \n",
       "67                      4.2  \n",
       "\n",
       "[68 rows x 5 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a63bac",
   "metadata": {},
   "source": [
    "En este código se están construyendo cuatro nuevas variables (features) basadas en patrones frecuentes de co-calificación entre películas, calculados con un enfoque al estilo Apriori. Cada una mide, para un usuario y una película objetivo, cuántos y cuánta “fuerza” de asociación tiene con otras películas que ese usuario ya calificó:\n",
    "\n",
    "* freq_pair_count\n",
    "\n",
    "Número de pares frecuentes que involucran a la película objetivo y otra película que el usuario ya calificó.\n",
    "\n",
    "Es decir, cuenta cuántas veces existe al menos una segunda película “other” tal que el par {película_objetivo, other} cumple con el soporte mínimo (min_support) en todo el dataset.\n",
    "\n",
    "* freq_pair_support_sum\n",
    "\n",
    "Suma de los valores de soporte de todos esos pares frecuentes.\n",
    "\n",
    "Cada par frecuente tiene un soporte support = (número_de_usuarios_que_calificaron_ambas) / total_de_usuarios.\n",
    "\n",
    "Al sumar esos soportes, capturamos no solo cuántos pares hay, sino también cuán “fuertes” o frecuentes son en la comunidad.\n",
    "\n",
    "* freq_triple_count\n",
    "\n",
    "Número de tríos frecuentes que involucran a la película objetivo y dos películas distintas que el usuario ya calificó.\n",
    "\n",
    "Cuenta cuántas combinaciones {película_objetivo, other1, other2} superan el umbral de soporte mínimo en el dataset.\n",
    "\n",
    "* freq_triple_support_sum\n",
    "\n",
    "Suma de los valores de soporte de todos esos tríos frecuentes.\n",
    "\n",
    "Aquí el soporte se calcula como (número_de_usuarios_que_calificaron_las_tres) / total_de_usuarios.\n",
    "\n",
    "Al agregar estos soportes, capturamos la fuerza de las asociaciones triples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10509a3b",
   "metadata": {},
   "source": [
    "**Apriori** → encontrar itemsets frecuentes\n",
    "\n",
    "**Feature crossing** → generar variables (freq_pair_count, freq_pair_support_sum, etc.) a partir de esos itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b38e8",
   "metadata": {},
   "source": [
    "---\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f1d7e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE en test: 1.4116\n",
      "Modelo guardado en 'rf_apriori_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import joblib  # para guardar/cargar el modelo\n",
    "\n",
    "# 1) Carga el dataset histórico\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2) Prepara usuarios por película y calcula soportes Apriori\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "total_users = df['userId'].nunique()\n",
    "min_support = 0.2\n",
    "\n",
    "frequent_pairs = {}\n",
    "frequent_triples = {}\n",
    "\n",
    "# Pares frecuentes\n",
    "for m1, m2 in combinations(users_by_movie, 2):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_pairs[frozenset({m1, m2})] = support\n",
    "\n",
    "# Tríos frecuentes\n",
    "for m1, m2, m3 in combinations(users_by_movie, 3):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_triples[frozenset({m1, m2, m3})] = support\n",
    "\n",
    "# 3) Función para generar las 4 variables Apriori\n",
    "def make_features(row):\n",
    "    user = row['userId']\n",
    "    movie = row['movieId']\n",
    "    history = set(df[df['userId']==user]['movieId']) - {movie}\n",
    "\n",
    "    pair_count = pair_sum = 0\n",
    "    triple_count = triple_sum = 0\n",
    "\n",
    "    # Pares\n",
    "    for other in history:\n",
    "        pair = frozenset({movie, other})\n",
    "        if pair in frequent_pairs:\n",
    "            pair_count += 1\n",
    "            pair_sum += frequent_pairs[pair]\n",
    "\n",
    "    # Tríos\n",
    "    for combo in combinations(history, 2):\n",
    "        triple = frozenset((movie,) + combo)\n",
    "        if triple in frequent_triples:\n",
    "            triple_count += 1\n",
    "            triple_sum += frequent_triples[triple]\n",
    "\n",
    "    return pd.Series({\n",
    "        'freq_pair_count': pair_count,\n",
    "        'freq_pair_support_sum': pair_sum,\n",
    "        'freq_triple_count': triple_count,\n",
    "        'freq_triple_support_sum': triple_sum\n",
    "    })\n",
    "\n",
    "# 4) Aplica sobre todo el histórico\n",
    "apriori_feats = df.apply(make_features, axis=1)\n",
    "df_feat = pd.concat([df, apriori_feats], axis=1)\n",
    "\n",
    "# 5) Prepara X, y y división train/test\n",
    "X = df_feat[['freq_pair_count', 'freq_pair_support_sum',\n",
    "             'freq_triple_count', 'freq_triple_support_sum']]\n",
    "y = df_feat['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6) Entrena el modelo\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7) Evalúa con RMSE\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE en test: {rmse:.4f}')\n",
    "\n",
    "# 8) Guarda el modelo para inferencia futura\n",
    "joblib.dump(model, 'rf_apriori_model.pkl')\n",
    "print(\"Modelo guardado en 'rf_apriori_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d390d88",
   "metadata": {},
   "source": [
    "---\n",
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ec0ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción de rating para user 7 y movie 5: 2.91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib  # para guardar/cargar tu modelo\n",
    "\n",
    "# --- 1) Reconstruir soportes Apriori (igual que en entrenamiento) ---\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "total_users = df['userId'].nunique()\n",
    "min_support = 0.2\n",
    "\n",
    "# pares y tríos frecuentes\n",
    "frequent_pairs = {}\n",
    "frequent_triples = {}\n",
    "\n",
    "for m1, m2 in combinations(users_by_movie, 2):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_pairs[frozenset({m1, m2})] = support\n",
    "\n",
    "for m1, m2, m3 in combinations(users_by_movie, 3):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_triples[frozenset({m1, m2, m3})] = support\n",
    "\n",
    "# --- 2) Función para extraer características dado un par (user, movie) ---\n",
    "def make_features(user_id, movie_id):\n",
    "    rated = set(df[df['userId']==user_id]['movieId']) - {movie_id}\n",
    "    pair_count = pair_sum = 0\n",
    "    triple_count = triple_sum = 0\n",
    "\n",
    "    # pares\n",
    "    for other in rated:\n",
    "        pair = frozenset({movie_id, other})\n",
    "        if pair in frequent_pairs:\n",
    "            pair_count += 1\n",
    "            pair_sum += frequent_pairs[pair]\n",
    "\n",
    "    # tríos\n",
    "    for combo in combinations(rated, 2):\n",
    "        triple = frozenset((movie_id,) + combo)\n",
    "        if triple in frequent_triples:\n",
    "            triple_count += 1\n",
    "            triple_sum += frequent_triples[triple]\n",
    "\n",
    "    return {\n",
    "        'freq_pair_count': pair_count,\n",
    "        'freq_pair_support_sum': pair_sum,\n",
    "        'freq_triple_count': triple_count,\n",
    "        'freq_triple_support_sum': triple_sum\n",
    "    }\n",
    "\n",
    "# --- 3) Carga tu modelo entrenado ---\n",
    "model: RandomForestRegressor = joblib.load(r\"C:\\Users\\juanj\\Desktop\\Primer test Apr Auto\\rf_apriori_model.pkl\")\n",
    "\n",
    "# --- 4) Predicción para un nuevo usuario y película ---\n",
    "new_user = 7\n",
    "new_movie = 5\n",
    "feat_dict = make_features(new_user, new_movie)\n",
    "X_new = pd.DataFrame([feat_dict])\n",
    "\n",
    "predicted_rating = model.predict(X_new)[0]\n",
    "print(f\"Predicción de rating para user {new_user} y movie {new_movie}: {predicted_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbc26c",
   "metadata": {},
   "source": [
    "---\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2704005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice original: 65\n",
      "Características de entrada: {'freq_pair_count': 5.0, 'freq_pair_support_sum': 2.0, 'freq_triple_count': 10.0, 'freq_triple_support_sum': 3.5999999999999996}\n",
      "Rating REAL: 5\n",
      "Rating PREDICHO: 4.6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 1) Carga del dataset y generación de características Apriori (como antes)\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "total_users = df['userId'].nunique()\n",
    "min_support = 0.2\n",
    "frequent_pairs, frequent_triples = {}, {}\n",
    "for m1, m2 in combinations(users_by_movie, 2):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2]\n",
    "    sup = len(inter) / total_users\n",
    "    if sup >= min_support:\n",
    "        frequent_pairs[frozenset({m1, m2})] = sup\n",
    "for m1, m2, m3 in combinations(users_by_movie, 3):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]\n",
    "    sup = len(inter) / total_users\n",
    "    if sup >= min_support:\n",
    "        frequent_triples[frozenset({m1, m2, m3})] = sup\n",
    "\n",
    "def make_features(row):\n",
    "    user,row_movie = row['userId'], row['movieId']\n",
    "    hist = set(df[df['userId']==user]['movieId']) - {row_movie}\n",
    "    pc = ps = tc = ts = 0\n",
    "    for other in hist:\n",
    "        p = frozenset({row_movie, other})\n",
    "        if p in frequent_pairs:\n",
    "            pc += 1; ps += frequent_pairs[p]\n",
    "    for combo in combinations(hist,2):\n",
    "        t = frozenset((row_movie,)+combo)\n",
    "        if t in frequent_triples:\n",
    "            tc += 1; ts += frequent_triples[t]\n",
    "    return pd.Series({\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    })\n",
    "\n",
    "feats = df.apply(make_features, axis=1)\n",
    "df_feat = pd.concat([df, feats], axis=1)\n",
    "X = df_feat[['freq_pair_count','freq_pair_support_sum',\n",
    "             'freq_triple_count','freq_triple_support_sum']]\n",
    "y = df_feat['rating']\n",
    "\n",
    "# 2) División y entrenamiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3) Seleccionar muestra aleatoria del test\n",
    "sample = X_test.sample(n=1, random_state=24)\n",
    "idx = sample.index[0]\n",
    "X_sample = sample\n",
    "y_actual = y_test.loc[idx]\n",
    "\n",
    "# 4) Predicción\n",
    "predicted = model.predict(X_sample)[0]\n",
    "\n",
    "# 5) Mostrar resultados\n",
    "print(f\"Índice original: {idx}\")\n",
    "print(\"Características de entrada:\", X_sample.to_dict(orient='records')[0])\n",
    "print(f\"Rating REAL: {y_actual}\")\n",
    "print(f\"Rating PREDICHO: {predicted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a0896075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE en test: 1.4116\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# 1) Carga del dataset y generación de características Apriori\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "total_users = df['userId'].nunique()\n",
    "min_support = 0.2\n",
    "\n",
    "frequent_pairs, frequent_triples = {}, {}\n",
    "for m1, m2 in combinations(users_by_movie, 2):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2]\n",
    "    sup = len(inter) / total_users\n",
    "    if sup >= min_support:\n",
    "        frequent_pairs[frozenset({m1, m2})] = sup\n",
    "for m1, m2, m3 in combinations(users_by_movie, 3):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]\n",
    "    sup = len(inter) / total_users\n",
    "    if sup >= min_support:\n",
    "        frequent_triples[frozenset({m1, m2, m3})] = sup\n",
    "\n",
    "def make_features(row):\n",
    "    user, movie = row['userId'], row['movieId']\n",
    "    hist = set(df[df['userId']==user]['movieId']) - {movie}\n",
    "    pc = ps = tc = ts = 0\n",
    "    for other in hist:\n",
    "        p = frozenset({movie, other})\n",
    "        if p in frequent_pairs:\n",
    "            pc += 1; ps += frequent_pairs[p]\n",
    "    for combo in combinations(hist, 2):\n",
    "        t = frozenset((movie,) + combo)\n",
    "        if t in frequent_triples:\n",
    "            tc += 1; ts += frequent_triples[t]\n",
    "    return pd.Series({\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    })\n",
    "\n",
    "feats = df.apply(make_features, axis=1)\n",
    "df_feat = pd.concat([df, feats], axis=1)\n",
    "\n",
    "# 2) Preparar X, y y división train/test\n",
    "X = df_feat[['freq_pair_count', 'freq_pair_support_sum',\n",
    "             'freq_triple_count', 'freq_triple_support_sum']]\n",
    "y = df_feat['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Entrenar el modelo\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4) Predecir en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5) Calcular RMSE\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE en test: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeccfb0",
   "metadata": {},
   "source": [
    "---\n",
    "# Mas features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8cc31",
   "metadata": {},
   "source": [
    "## RandomForestRegressor Geners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d24342c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE RandomForest: 1.1863\n",
      "RMSE HistGradientBoosting: 1.3197\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Usuario-por-película y soportes\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u) / total_users for m, u in users_by_movie.items()}\n",
    "\n",
    "# 3. Generar ítems frecuentes de tamaño 2 y 3 (soporte ≥ 0.2)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {frozenset({m1, m2}): len(users_by_movie[m1] & users_by_movie[m2]) / total_users\n",
    "                  for m1, m2 in combinations(users_by_movie.keys(), 2)\n",
    "                  if len(users_by_movie[m1] & users_by_movie[m2]) / total_users >= min_support}\n",
    "frequent_triples = {frozenset({m1, m2, m3}):\n",
    "                    len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users\n",
    "                    for m1, m2, m3 in combinations(users_by_movie.keys(), 3)\n",
    "                    if len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users >= min_support}\n",
    "\n",
    "# 4. Extracción de features avanzadas\n",
    "def apriori_features_ultimate(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "    \n",
    "    # Unarios\n",
    "    sup_target = movie_support.get(target, 0.0)\n",
    "    cnt_rated = len(rated)\n",
    "    \n",
    "    # Inicializar\n",
    "    pair_supports = []\n",
    "    pair_leverages = []\n",
    "    confs = []\n",
    "    lifts = []\n",
    "    weighted_ratings = []\n",
    "    \n",
    "    triple_supports = []\n",
    "    triple_leverages = []\n",
    "    triple_lifts = []\n",
    "    \n",
    "    for other in rated:\n",
    "        pair = frozenset({target, other})\n",
    "        if pair in frequent_pairs:\n",
    "            sup = frequent_pairs[pair]\n",
    "            pair_supports.append(sup)\n",
    "            t_sup = sup_target\n",
    "            o_sup = movie_support.get(other, 0)\n",
    "            pair_leverages.append(sup - t_sup * o_sup)\n",
    "            if t_sup > 0:\n",
    "                confs.append(sup / t_sup)\n",
    "            if t_sup > 0 and o_sup > 0:\n",
    "                lifts.append(sup / (t_sup * o_sup))\n",
    "            # Weighted by support\n",
    "            rating_other = df[(df['userId'] == user) & (df['movieId'] == other)]['rating'].iloc[0]\n",
    "            weighted_ratings.append(sup * rating_other)\n",
    "    \n",
    "    for combo in combinations(rated, 2):\n",
    "        triple = frozenset((target,) + combo)\n",
    "        if triple in frequent_triples:\n",
    "            sup3 = frequent_triples[triple]\n",
    "            triple_supports.append(sup3)\n",
    "            t_sup = sup_target\n",
    "            o1_sup = movie_support.get(combo[0], 0)\n",
    "            o2_sup = movie_support.get(combo[1], 0)\n",
    "            triple_leverages.append(sup3 - t_sup * o1_sup * o2_sup)\n",
    "            if t_sup > 0 and o1_sup > 0 and o2_sup > 0:\n",
    "                triple_lifts.append(sup3 / (t_sup * o1_sup * o2_sup))\n",
    "    \n",
    "    # Features cálculo\n",
    "    return pd.Series({\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': cnt_rated,\n",
    "        'freq_pair_count': len(pair_supports),\n",
    "        'freq_pair_support_sum': sum(pair_supports),\n",
    "        'max_pair_support': max(pair_supports) if pair_supports else 0.0,\n",
    "        'min_pair_support': min(pair_supports) if pair_supports else 0.0,\n",
    "        'avg_pair_support': sum(pair_supports)/len(pair_supports) if pair_supports else 0.0,\n",
    "        'sum_pair_leverage': sum(pair_leverages),\n",
    "        'max_pair_leverage': max(pair_leverages) if pair_leverages else 0.0,\n",
    "        'max_pair_confidence': max(confs) if confs else 0.0,\n",
    "        'avg_pair_lift': sum(lifts)/len(lifts) if lifts else 0.0,\n",
    "        'max_pair_lift': max(lifts) if lifts else 0.0,\n",
    "        'weighted_avg_rating_pair': sum(weighted_ratings)/sum(pair_supports) if pair_supports else 0.0,\n",
    "        'freq_triple_count': len(triple_supports),\n",
    "        'freq_triple_support_sum': sum(triple_supports),\n",
    "        'avg_triple_support': sum(triple_supports)/len(triple_supports) if triple_supports else 0.0,\n",
    "        'max_triple_support': max(triple_supports) if triple_supports else 0.0,\n",
    "        'sum_triple_leverage': sum(triple_leverages),\n",
    "        'max_triple_lift': max(triple_lifts) if triple_lifts else 0.0,\n",
    "        'avg_triple_lift': sum(triple_lifts)/len(triple_lifts) if triple_lifts else 0.0,\n",
    "        'triple_coverage': len(triple_supports)/ (cnt_rated*(cnt_rated-1)/2) if cnt_rated > 1 else 0.0\n",
    "    })\n",
    "\n",
    "# 5. Generar dataset final\n",
    "ultimate_feats = df.apply(apriori_features_ultimate, axis=1)\n",
    "df_ult = pd.concat([df, ultimate_feats], axis=1).drop(['userId','movieId','timestamp'], axis=1)\n",
    "X = df_ult.drop('rating', axis=1)\n",
    "y = df_ult['rating']\n",
    "\n",
    "# 6. Dividir y entrenar modelos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest optimizado\n",
    "rf = RandomForestRegressor(n_estimators=549, max_depth=5, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "rmse_rf = sqrt(mean_squared_error(y_test, pred_rf))\n",
    "\n",
    "# HistGradientBoosting\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "hgb.fit(X_train, y_train)\n",
    "pred_hgb = hgb.predict(X_test)\n",
    "rmse_hgb = sqrt(mean_squared_error(y_test, pred_hgb))\n",
    "\n",
    "print(f\"RMSE RandomForest: {rmse_rf:.4f}\")\n",
    "print(f\"RMSE HistGradientBoosting: {rmse_hgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cb22c",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b752d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE XGBoost: 1.6048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Usuario-por-película y soporte global\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u) / total_users for m, u in users_by_movie.items()}\n",
    "\n",
    "# 3. Generar ítems frecuentes de tamaño 2 y 3 (soporte ≥ 0.2)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset([m1, m2]): len(users_by_movie[m1] & users_by_movie[m2]) / total_users\n",
    "    for m1, m2 in combinations(users_by_movie.keys(), 2)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1, m2, m3]): len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users\n",
    "    for m1, m2, m3 in combinations(users_by_movie.keys(), 3)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Función Apriori para pares y tríos\n",
    "def apriori_features(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "    pc = ps = tc = ts = 0\n",
    "    for other in rated:\n",
    "        pair = frozenset([target, other])\n",
    "        if pair in frequent_pairs:\n",
    "            pc += 1\n",
    "            ps += frequent_pairs[pair]\n",
    "    for combo in combinations(rated, 2):\n",
    "        tri = frozenset([target] + list(combo))\n",
    "        if tri in frequent_triples:\n",
    "            tc += 1\n",
    "            ts += frequent_triples[tri]\n",
    "    return pd.Series({\n",
    "        'sup_target': movie_support.get(target, 0.0),\n",
    "        'cnt_rated': len(rated),\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    })\n",
    "\n",
    "# 5. Aplicar Apriori\n",
    "apriori_feats = df.apply(apriori_features, axis=1)\n",
    "df_feat = pd.concat([df, apriori_feats], axis=1)\n",
    "\n",
    "# 6. Matriz usuario×película y SVD\n",
    "user_item = df.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "n_comp = min(20, user_item.shape[1] - 1, user_item.shape[0] - 1)\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "user_latent = svd.fit_transform(user_item)\n",
    "movie_latent = svd.components_.T\n",
    "\n",
    "user_latent_df = pd.DataFrame(user_latent, index=user_item.index,\n",
    "                              columns=[f'u_lat_{i}' for i in range(n_comp)])\n",
    "movie_latent_df = pd.DataFrame(movie_latent, index=user_item.columns,\n",
    "                               columns=[f'm_lat_{i}' for i in range(n_comp)])\n",
    "\n",
    "df_feat = df_feat.merge(user_latent_df, left_on='userId', right_index=True)\n",
    "df_feat = df_feat.merge(movie_latent_df, left_on='movieId', right_index=True)\n",
    "\n",
    "# 7. Preparar X, y\n",
    "df_final = df_feat.drop(['userId', 'movieId', 'timestamp'], axis=1)\n",
    "X = df_final.drop('rating', axis=1)\n",
    "y = df_final['rating']\n",
    "\n",
    "# 8. División\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 9. Entrenar XGBoost\n",
    "xgb = XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "                   objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "rmse_xgb = sqrt(mean_squared_error(y_test, pred_xgb))\n",
    "\n",
    "print(f\"RMSE XGBoost: {rmse_xgb:.4f}\")\n",
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b0afb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE XGBoost con 9 features Apriori: 1.6430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# Pre-cálculo de soportes\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u) / total_users for m, u in users_by_movie.items()}\n",
    "\n",
    "# Soporte mínimo y generación de pares y tríos frecuentes\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset({m1, m2}): len(users_by_movie[m1] & users_by_movie[m2]) / total_users\n",
    "    for m1, m2 in combinations(users_by_movie.keys(), 2)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset({m1, m2, m3}):\n",
    "    len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users\n",
    "    for m1, m2, m3 in combinations(users_by_movie.keys(), 3)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# Función de extracción de 9 features Apriori\n",
    "def apriori_features(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "    \n",
    "    pair_count = pair_sum = 0\n",
    "    confidences = []\n",
    "    lifts = []\n",
    "    triple_count = triple_sum = 0\n",
    "    \n",
    "    sup_target = movie_support.get(target, 0.0)\n",
    "    \n",
    "    for other in rated:\n",
    "        pair = frozenset({target, other})\n",
    "        if pair in frequent_pairs:\n",
    "            sup = frequent_pairs[pair]\n",
    "            pair_count += 1\n",
    "            pair_sum += sup\n",
    "            if sup_target > 0:\n",
    "                confidences.append(sup / sup_target)\n",
    "            o_sup = movie_support.get(other, 0)\n",
    "            if sup_target > 0 and o_sup > 0:\n",
    "                lifts.append(sup / (sup_target * o_sup))\n",
    "    \n",
    "    for combo in combinations(rated, 2):\n",
    "        triple = frozenset({target, *combo})\n",
    "        if triple in frequent_triples:\n",
    "            triple_count += 1\n",
    "            triple_sum += frequent_triples[triple]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': len(rated),\n",
    "        'freq_pair_count': pair_count,\n",
    "        'freq_pair_support_sum': pair_sum,\n",
    "        'max_pair_confidence': max(confidences) if confidences else 0.0,\n",
    "        'avg_pair_lift': sum(lifts)/len(lifts) if lifts else 0.0,\n",
    "        'freq_triple_count': triple_count,\n",
    "        'freq_triple_support_sum': triple_sum,\n",
    "        'avg_triple_lift': sum(lifts)/len(lifts) if lifts else 0.0\n",
    "    })\n",
    "\n",
    "# Generar features Apriori\n",
    "apriori_feats = df.apply(apriori_features, axis=1)\n",
    "df_feat = pd.concat([df, apriori_feats], axis=1)\n",
    "\n",
    "# Preparar X, y\n",
    "df_final = df_feat.drop(['userId','movieId','timestamp'], axis=1)\n",
    "X = df_final.drop('rating', axis=1)\n",
    "y = df_final['rating']\n",
    "\n",
    "# División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar XGBRegressor\n",
    "xgb = XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "                   objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse_xgb = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE XGBoost con 9 features Apriori: {rmse_xgb:.4f}\")\n",
    "len(X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6630f5",
   "metadata": {},
   "source": [
    "## FactorizationMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3af8f373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE con FM PyTorch: 1.5279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1. Carga y features base (idéntico a tu código)\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "df['user_avg_rating'] = df.groupby('userId')['rating'].transform('mean')\n",
    "df['movie_avg_rating'] = df.groupby('movieId')['rating'].transform('mean')\n",
    "df['rating_diff'] = df['user_avg_rating'] - df['movie_avg_rating']\n",
    "\n",
    "# 2. Apriori básico (idéntico)\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "min_support = 0.2\n",
    "\n",
    "frequent_pairs = {}\n",
    "for m1, m2 in combinations(users_by_movie, 2):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_pairs[frozenset({m1, m2})] = support\n",
    "\n",
    "frequent_triples = {}\n",
    "for m1, m2, m3 in combinations(users_by_movie, 3):\n",
    "    inter = users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]\n",
    "    support = len(inter) / total_users\n",
    "    if support >= min_support:\n",
    "        frequent_triples[frozenset({m1, m2, m3})] = support\n",
    "\n",
    "def apriori_basic(row):\n",
    "    user, target = row['userId'], row['movieId']\n",
    "    rated = set(df[df['userId']==user]['movieId']) - {target}\n",
    "    pc = ps = tc = ts = 0\n",
    "    for other in rated:\n",
    "        pair = frozenset({target, other})\n",
    "        if pair in frequent_pairs:\n",
    "            pc += 1\n",
    "            ps += frequent_pairs[pair]\n",
    "    for combo in combinations(rated, 2):\n",
    "        tri = frozenset((target,)+combo)\n",
    "        if tri in frequent_triples:\n",
    "            tc += 1\n",
    "            ts += frequent_triples[tri]\n",
    "    return pd.Series({\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    })\n",
    "\n",
    "apriori_feats = df.apply(apriori_basic, axis=1)\n",
    "df_model = pd.concat([df, apriori_feats], axis=1)\n",
    "\n",
    "# 3. Preparamos tensores para PyTorch\n",
    "X = df_model[['user_avg_rating','movie_avg_rating','rating_diff',\n",
    "              'freq_pair_count','freq_pair_support_sum',\n",
    "              'freq_triple_count','freq_triple_support_sum']].values\n",
    "y = df_model['rating'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convertimos a tensores\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t  = torch.FloatTensor(X_test)\n",
    "y_test_t  = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_t, y_train_t),\n",
    "    batch_size=1024, shuffle=True\n",
    ")\n",
    "\n",
    "# 4. Definimos el modelo FM\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, n_features, k):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "        # Factores latentes V: [n_features × k]\n",
    "        self.v = nn.Parameter(torch.randn(n_features, k) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # parte lineal\n",
    "        lin = self.linear(x)  # [batch,1]\n",
    "        # interacción de segundo orden:\n",
    "        # ( (xV)^2 - (x^2 V^2) ).sum(dim=1) * 0.5\n",
    "        xv = x @ self.v              # [batch, k]\n",
    "        xv2 = (x**2) @ (self.v**2)   # [batch, k]\n",
    "        interactions = 0.5 * torch.sum(xv**2 - xv2, dim=1, keepdim=True)\n",
    "        return lin + interactions\n",
    "\n",
    "# 5. Entrenamiento\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FactorizationMachine(n_features=X_train.shape[1], k=10).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        preds = model(xb).squeeze()\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    #print(f\"Epoch {epoch+1:02d}  MSE train: {total_loss/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "# 6. Evaluación\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(X_test_t.to(device)).squeeze().cpu().numpy()\n",
    "rmse = sqrt(mean_squared_error(y_test, preds_test))\n",
    "print(f\"\\nRMSE con FM PyTorch: {rmse:.4f}\")\n",
    "len(X_train[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffed2c",
   "metadata": {},
   "source": [
    "## RandomForestRegressor Geners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "920d2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE con géneros + Apriori: 1.7374\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Leer ratings y movies (con géneros)\n",
    "df_ratings = pd.read_csv('ratings2comoML.csv')\n",
    "df_movies  = pd.read_csv(r\"C:\\Users\\juanj\\Desktop\\ml-32m\\movies.csv\")  # movieId,title,genres\n",
    "\n",
    "# 2. Preprocesar géneros: one-hot encoding\n",
    "#    Cada película puede tener múltiples géneros separados por '|'\n",
    "genres_expanded = df_movies['genres'].str.get_dummies(sep='|')\n",
    "df_genres = pd.concat([df_movies[['movieId']], genres_expanded], axis=1)\n",
    "\n",
    "# 3. Calcular Apriori features (pares + tríos) — igual que tu código\n",
    "total_users = df_ratings['userId'].nunique()\n",
    "users_by_movie = df_ratings.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u)/total_users for m,u in users_by_movie.items()}\n",
    "min_support = 0.2\n",
    "\n",
    "# ítems frecuentes\n",
    "frequent_pairs = {\n",
    "    frozenset([m1,m2]): len(users_by_movie[m1]&users_by_movie[m2]) / total_users\n",
    "    for m1,m2 in combinations(users_by_movie,2)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1,m2,m3]):\n",
    "        len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users\n",
    "    for m1,m2,m3 in combinations(users_by_movie,3)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "def apriori_feats(row):\n",
    "    user,target = row['userId'], row['movieId']\n",
    "    rated = set(df_ratings[df_ratings['userId']==user]['movieId']) - {target}\n",
    "    pc = ps = tc = ts = 0\n",
    "    for other in rated:\n",
    "        p = frozenset([target,other])\n",
    "        if p in frequent_pairs:\n",
    "            pc += 1\n",
    "            ps += frequent_pairs[p]\n",
    "    for combo in combinations(rated,2):\n",
    "        t = frozenset([target,*combo])\n",
    "        if t in frequent_triples:\n",
    "            tc += 1\n",
    "            ts += frequent_triples[t]\n",
    "    return pd.Series({\n",
    "        'sup_target': movie_support.get(target,0),\n",
    "        'cnt_rated': len(rated),\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    })\n",
    "\n",
    "# 4. Construir DF con features Apriori\n",
    "apriori_features = df_ratings.apply(apriori_feats, axis=1)\n",
    "df = pd.concat([df_ratings, apriori_features], axis=1)\n",
    "\n",
    "# 5. Merge con géneros\n",
    "df = df.merge(df_genres, on='movieId')\n",
    "\n",
    "# 6. Preparar X e y\n",
    "X = df.drop(['rating', 'userId', 'movieId', 'timestamp', 'title'] if 'title' in df.columns else ['rating','userId','movieId','timestamp'], axis=1)\n",
    "y = df['rating']\n",
    "\n",
    "# 7. Train/Test split y entrenamiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=549, max_depth=5, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "# 8. Evaluación\n",
    "rmse = sqrt(mean_squared_error(y_test, pred))\n",
    "print(f\"RMSE con géneros + Apriori: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41665ede",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "87674192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE XGBoost (Apriori ultimate): 1.1876\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1. Carga del dataset de ratings\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Soporte global y sets de usuarios por película\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u)/total_users for m,u in users_by_movie.items()}\n",
    "\n",
    "# 3. Generar ítems frecuentes de tamaño 2 y 3\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset([m1,m2]): len(users_by_movie[m1]&users_by_movie[m2])/total_users\n",
    "    for m1,m2 in combinations(users_by_movie,2)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2])/total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1,m2,m3]):\n",
    "    len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3])/total_users\n",
    "    for m1,m2,m3 in combinations(users_by_movie,3)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3])/total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Función Apriori features ultimate\n",
    "def apriori_features_ultimate(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId']==user]['movieId']) - {target}\n",
    "    \n",
    "    sup_target = movie_support.get(target, 0.0)\n",
    "    cnt_rated = len(rated)\n",
    "    \n",
    "    pair_supports = []\n",
    "    pair_leverages = []\n",
    "    confs = []\n",
    "    lifts = []\n",
    "    weighted_ratings = []\n",
    "    triple_supports = []\n",
    "    triple_leverages = []\n",
    "    triple_lifts = []\n",
    "    \n",
    "    for other in rated:\n",
    "        p = frozenset([target,other])\n",
    "        if p in frequent_pairs:\n",
    "            sup = frequent_pairs[p]\n",
    "            pair_supports.append(sup)\n",
    "            o_sup = movie_support.get(other, 0)\n",
    "            pair_leverages.append(sup - sup_target*o_sup)\n",
    "            if sup_target>0:\n",
    "                confs.append(sup/sup_target)\n",
    "            if sup_target>0 and o_sup>0:\n",
    "                lifts.append(sup/(sup_target*o_sup))\n",
    "            rating_other = df[(df['userId']==user)&(df['movieId']==other)]['rating'].iloc[0]\n",
    "            weighted_ratings.append(sup*rating_other)\n",
    "    for combo in combinations(rated,2):\n",
    "        t = frozenset([target,*combo])\n",
    "        if t in frequent_triples:\n",
    "            sup3 = frequent_triples[t]\n",
    "            triple_supports.append(sup3)\n",
    "            o1,o2 = combo\n",
    "            o1_sup = movie_support.get(o1,0)\n",
    "            o2_sup = movie_support.get(o2,0)\n",
    "            triple_leverages.append(sup3 - sup_target*o1_sup*o2_sup)\n",
    "            if sup_target>0 and o1_sup>0 and o2_sup>0:\n",
    "                triple_lifts.append(sup3/(sup_target*o1_sup*o2_sup))\n",
    "    return pd.Series({\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': cnt_rated,\n",
    "        'freq_pair_count': len(pair_supports),\n",
    "        'freq_pair_support_sum': sum(pair_supports),\n",
    "        'max_pair_support': max(pair_supports) if pair_supports else 0,\n",
    "        'min_pair_support': min(pair_supports) if pair_supports else 0,\n",
    "        'avg_pair_support': sum(pair_supports)/len(pair_supports) if pair_supports else 0,\n",
    "        'sum_pair_leverage': sum(pair_leverages),\n",
    "        'max_pair_leverage': max(pair_leverages) if pair_leverages else 0,\n",
    "        'max_pair_confidence': max(confs) if confs else 0,\n",
    "        'avg_pair_lift': sum(lifts)/len(lifts) if lifts else 0,\n",
    "        'max_pair_lift': max(lifts) if lifts else 0,\n",
    "        'weighted_avg_rating_pair': sum(weighted_ratings)/sum(pair_supports) if pair_supports else 0,\n",
    "        'freq_triple_count': len(triple_supports),\n",
    "        'freq_triple_support_sum': sum(triple_supports),\n",
    "        'avg_triple_support': sum(triple_supports)/len(triple_supports) if triple_supports else 0,\n",
    "        'max_triple_support': max(triple_supports) if triple_supports else 0,\n",
    "        'sum_triple_leverage': sum(triple_leverages),\n",
    "        'max_triple_lift': max(triple_lifts) if triple_lifts else 0,\n",
    "        'avg_triple_lift': sum(triple_lifts)/len(triple_lifts) if triple_lifts else 0,\n",
    "        'triple_coverage': len(triple_supports)/(cnt_rated*(cnt_rated-1)/2) if cnt_rated>1 else 0\n",
    "    })\n",
    "\n",
    "# 5. Aplicar y preparar dataset\n",
    "feat_df = df.apply(apriori_features_ultimate, axis=1)\n",
    "df_ml = pd.concat([df, feat_df], axis=1)\n",
    "X = df_ml.drop(['userId','movieId','timestamp','rating'], axis=1)\n",
    "y = df_ml['rating']\n",
    "\n",
    "# 6. División\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Entrenar XGBoost con parámetros ajustados\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    n_jobs=4\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluar RMSE\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE XGBoost (Apriori ultimate): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c515632",
   "metadata": {},
   "source": [
    "## XGBoost Geners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cf3ab5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE XGBoost (Apriori + Géneros): 1.9367\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1. Carga de datasets con bajo uso de memoria\n",
    "df_ratings = pd.read_csv('ratings2comoML.csv')\n",
    "df_movies = pd.read_csv('movies.csv', usecols=['movieId','genres'])\n",
    "\n",
    "# 2. Preprocesar géneros manualmente\n",
    "movie_genres = df_movies.set_index('movieId')['genres']\\\n",
    "                .apply(lambda s: s.split('|') if isinstance(s,str) else []).to_dict()\n",
    "all_genres = sorted({g for genres in movie_genres.values() for g in genres})\n",
    "\n",
    "# 3. Apriori: soporte global y frecuentes\n",
    "total_users = df_ratings['userId'].nunique()\n",
    "users_by_movie = df_ratings.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u)/total_users for m,u in users_by_movie.items()}\n",
    "min_support = 0.2\n",
    "\n",
    "# Pares/tríos frecuentes\n",
    "frequent_pairs = {\n",
    "    frozenset([m1,m2]): len(users_by_movie[m1]&users_by_movie[m2])/total_users\n",
    "    for m1,m2 in combinations(users_by_movie,2)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2])/total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1,m2,m3]):\n",
    "    len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3])/total_users\n",
    "    for m1,m2,m3 in combinations(users_by_movie,3)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3])/total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Función de features Apriori + géneros\n",
    "def apriori_feats(row):\n",
    "    user, target = row['userId'], row['movieId']\n",
    "    rated = set(df_ratings[df_ratings['userId']==user]['movieId']) - {target}\n",
    "    pc = ps = 0\n",
    "    tc = ts = 0\n",
    "    sup_target = movie_support.get(target,0.0)\n",
    "    for other in rated:\n",
    "        p = frozenset([target, other])\n",
    "        if p in frequent_pairs:\n",
    "            sup = frequent_pairs[p]\n",
    "            pc += 1; ps += sup\n",
    "    for combo in combinations(rated,2):\n",
    "        t = frozenset([target,*combo])\n",
    "        if t in frequent_triples:\n",
    "            tc += 1; ts += frequent_triples[t]\n",
    "    feats = {\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': len(rated),\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    }\n",
    "    # añadir géneros one-hot\n",
    "    genres = movie_genres.get(target, [])\n",
    "    for g in all_genres:\n",
    "        feats[f'genre_{g}'] = int(g in genres)\n",
    "    return pd.Series(feats)\n",
    "\n",
    "# 5. Generar features y merge\n",
    "apriori_df = df_ratings.apply(apriori_feats, axis=1)\n",
    "df_ml = pd.concat([df_ratings, apriori_df], axis=1)\n",
    "\n",
    "# 6. Preparar X,e y\n",
    "drop_cols = ['userId','movieId','timestamp']\n",
    "X = df_ml.drop(drop_cols + ['rating'], axis=1)\n",
    "y = df_ml['rating']\n",
    "\n",
    "# 7. Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 8. Entrenar XGBoost\n",
    "xgb = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.05,\n",
    "                   subsample=0.8, colsample_bytree=0.8,\n",
    "                   objective='reg:squarederror', tree_method='hist',\n",
    "                   random_state=42, n_jobs=4)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 9. Evaluación\n",
    "y_pred = xgb.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE XGBoost (Apriori + Géneros): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b92e80",
   "metadata": {},
   "source": [
    "## Géneros + SVD + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "645a67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Apriori + Géneros + SVD + XGBoost: 1.8158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Carga de datos\n",
    "df_ratings = pd.read_csv('ratings2comoML.csv')\n",
    "df_movies  = pd.read_csv('movies.csv')   # columnas: movieId, title, genres\n",
    "\n",
    "# 2. One-hot encoding de géneros\n",
    "df_genres = df_movies['genres'].str.get_dummies(sep='|')\n",
    "df_genres['movieId'] = df_movies['movieId']\n",
    "\n",
    "# 3. Pre-cálculo de soportes para Apriori\n",
    "total_users = df_ratings['userId'].nunique()\n",
    "users_by_movie = df_ratings.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support   = {m: len(u)/total_users for m,u in users_by_movie.items()}\n",
    "min_support     = 0.2\n",
    "\n",
    "frequent_pairs = {\n",
    "    frozenset([m1,m2]): len(users_by_movie[m1]&users_by_movie[m2]) / total_users\n",
    "    for m1,m2 in combinations(users_by_movie,2)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1,m2,m3]):\n",
    "      len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users\n",
    "    for m1,m2,m3 in combinations(users_by_movie,3)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Función para extraer features Apriori (pares + tríos)\n",
    "def apriori_features(row):\n",
    "    u, target = row['userId'], row['movieId']\n",
    "    seen = set(df_ratings[df_ratings['userId']==u]['movieId']) - {target}\n",
    "    pc = ps = tc = ts = 0\n",
    "    sup_t = movie_support.get(target, 0.0)\n",
    "    for other in seen:\n",
    "        p = frozenset([target, other])\n",
    "        if p in frequent_pairs:\n",
    "            pc += 1\n",
    "            ps += frequent_pairs[p]\n",
    "    for a,b in combinations(seen,2):\n",
    "        t = frozenset([target,a,b])\n",
    "        if t in frequent_triples:\n",
    "            tc += 1\n",
    "            ts += frequent_triples[t]\n",
    "    return pd.Series({\n",
    "        'sup_target': sup_t,\n",
    "        'cnt_rated': len(seen),\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts\n",
    "    })\n",
    "\n",
    "# 5. Aplicar Apriori y merge con géneros\n",
    "apr_feats = df_ratings.apply(apriori_features, axis=1)\n",
    "df = pd.concat([df_ratings, apr_feats], axis=1)\n",
    "df = df.merge(df_genres, on='movieId', how='left')\n",
    "\n",
    "# 6. Construir matriz usuario×película y extraer factores latentes\n",
    "ui = df_ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "k = min(20, ui.shape[1]-1, ui.shape[0]-1)\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "U = svd.fit_transform(ui)             # (n_users, k)\n",
    "V = svd.components_.T                # (n_movies, k)\n",
    "\n",
    "df_U = pd.DataFrame(U, index=ui.index, columns=[f'u_lat_{i}' for i in range(k)])\n",
    "df_V = pd.DataFrame(V, index=ui.columns, columns=[f'm_lat_{i}' for i in range(k)])\n",
    "\n",
    "df = df.merge(df_U, left_on='userId',  right_index=True, how='left')\n",
    "df = df.merge(df_V, left_on='movieId', right_index=True, how='left')\n",
    "\n",
    "# 7. Preparar X e y\n",
    "drop_cols = ['userId','movieId','timestamp','rating','title']\n",
    "X = df.drop([c for c in drop_cols if c in df.columns], axis=1)\n",
    "y = df['rating']\n",
    "\n",
    "# 8. División y entrenamiento con XGBoost\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "# 9. Predicción y cálculo de RMSE\n",
    "y_pred = model.predict(X_te)\n",
    "rmse = sqrt(mean_squared_error(y_te, y_pred))\n",
    "print(f\"RMSE Apriori + Géneros + SVD + XGBoost: {rmse:.4f}\")\n",
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efdd32",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b351a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE KNN: 1.9882\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Pre-cálculo de soportes y sets de usuarios por película\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u) / total_users for m, u in users_by_movie.items()}\n",
    "\n",
    "# 3. Ítems frecuentes de tamaño 2 y 3 (soporte ≥ 0.2)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset([m1, m2]): len(users_by_movie[m1] & users_by_movie[m2]) / total_users\n",
    "    for m1, m2 in combinations(users_by_movie, 2)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1, m2, m3]): len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users\n",
    "    for m1, m2, m3 in combinations(users_by_movie, 3)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Definición de la función de extracción de features Apriori\n",
    "def apriori_features_ultimate(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "    \n",
    "    sup_target = movie_support.get(target, 0.0)\n",
    "    rated_count = len(rated)\n",
    "    \n",
    "    pair_count = pair_sum = 0\n",
    "    triple_count = triple_sum = 0\n",
    "    \n",
    "    for other in rated:\n",
    "        pair = frozenset([target, other])\n",
    "        if pair in frequent_pairs:\n",
    "            sup = frequent_pairs[pair]\n",
    "            pair_count += 1\n",
    "            pair_sum += sup\n",
    "    \n",
    "    for combo in combinations(rated, 2):\n",
    "        tri = frozenset([target, *combo])\n",
    "        if tri in frequent_triples:\n",
    "            sup3 = frequent_triples[tri]\n",
    "            triple_count += 1\n",
    "            triple_sum += sup3\n",
    "    \n",
    "    return pd.Series({\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': rated_count,\n",
    "        'freq_pair_count': pair_count,\n",
    "        'freq_pair_support_sum': pair_sum,\n",
    "        'freq_triple_count': triple_count,\n",
    "        'freq_triple_support_sum': triple_sum\n",
    "    })\n",
    "\n",
    "# 5. Aplicar extracción de features Apriori\n",
    "apr_feats = df.apply(apriori_features_ultimate, axis=1)\n",
    "df_ml = pd.concat([df, apr_feats], axis=1)\n",
    "\n",
    "# 6. Preparar X e y\n",
    "X = df_ml.drop(['userId', 'movieId', 'timestamp', 'rating'], axis=1)\n",
    "y = df_ml['rating']\n",
    "\n",
    "# 7. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 8. Pipeline: escalado + KNN\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(\n",
    "        n_neighbors=10,\n",
    "        weights='distance',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 9. Entrenamiento y predicción\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# 10. Evaluación RMSE\n",
    "rmse_knn = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE KNN: {rmse_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952da926",
   "metadata": {},
   "source": [
    "## KNN Geners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "363d1f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE KNN (Apriori+Géneros, 10 vars): 2.0835\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Carga de dataset de ratings\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Carga de movies.csv con géneros\n",
    "df_movies = pd.read_csv('movies.csv', usecols=['movieId','genres'], engine='python')\n",
    "movie_genres = df_movies.set_index('movieId')['genres'].str.split('|').to_dict()\n",
    "\n",
    "# 3. Pre-cálculo de soporte global y sets de usuarios por película\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u) / total_users for m, u in users_by_movie.items()}\n",
    "\n",
    "# 4. Generar ítems frecuentes de tamaño 2 y 3 (soporte ≥ 0.2)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset([m1, m2]): len(users_by_movie[m1] & users_by_movie[m2]) / total_users\n",
    "    for m1, m2 in combinations(users_by_movie, 2)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1, m2, m3]):\n",
    "    len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users\n",
    "    for m1, m2, m3 in combinations(users_by_movie, 3)\n",
    "    if len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# 5. Función para extraer features Apriori + géneros\n",
    "def extract_features(row):\n",
    "    user, target = row['userId'], row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "    \n",
    "    sup_target = movie_support.get(target, 0.0)\n",
    "    cnt_rated = len(rated)\n",
    "    pair_count = pair_sum = 0\n",
    "    triple_count = triple_sum = 0\n",
    "    \n",
    "    # Pares frecuentes\n",
    "    for other in rated:\n",
    "        p = frozenset([target, other])\n",
    "        if p in frequent_pairs:\n",
    "            s = frequent_pairs[p]\n",
    "            pair_count += 1\n",
    "            pair_sum += s\n",
    "    \n",
    "    # Tríos frecuentes\n",
    "    for combo in combinations(rated, 2):\n",
    "        t = frozenset([target, *combo])\n",
    "        if t in frequent_triples:\n",
    "            s3 = frequent_triples[t]\n",
    "            triple_count += 1\n",
    "            triple_sum += s3\n",
    "    \n",
    "    feats = {\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': cnt_rated,\n",
    "        'freq_pair_count': pair_count,\n",
    "        'freq_pair_support_sum': pair_sum,\n",
    "        'freq_triple_count': triple_count,\n",
    "        'freq_triple_support_sum': triple_sum\n",
    "    }\n",
    "    \n",
    "    # Features de géneros\n",
    "    genres = movie_genres.get(target, [])\n",
    "    feats['num_genres'] = len(genres)\n",
    "    feats['is_genre_Comedy'] = int('Comedy' in genres)\n",
    "    feats['is_genre_Drama'] = int('Drama' in genres)\n",
    "    feats['is_genre_Action'] = int('Action' in genres)\n",
    "    \n",
    "    return pd.Series(feats)\n",
    "\n",
    "# 6. Aplicar extracción y preparar X, y\n",
    "features = df.apply(extract_features, axis=1)\n",
    "df_ml = pd.concat([df, features], axis=1)\n",
    "X = df_ml.drop(['userId', 'movieId', 'timestamp', 'rating'], axis=1)\n",
    "y = df_ml['rating']\n",
    "\n",
    "# 7. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8. Pipeline KNN: escalado + KNeighborsRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=10, weights='distance', n_jobs=1))\n",
    "])\n",
    "\n",
    "# 9. Entrenar y predecir\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# 10. Evaluación RMSE\n",
    "rmse_knn = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE KNN (Apriori+Géneros, 10 vars): {rmse_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b69cab5",
   "metadata": {},
   "source": [
    "---\n",
    "# NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8c4a7",
   "metadata": {},
   "source": [
    "## NN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d1c1a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — MSE train: 17.3046\n",
      "Epoch 02 — MSE train: 17.0984\n",
      "Epoch 03 — MSE train: 16.8921\n",
      "Epoch 04 — MSE train: 16.6835\n",
      "Epoch 05 — MSE train: 16.4706\n",
      "Epoch 06 — MSE train: 16.2515\n",
      "Epoch 07 — MSE train: 16.0243\n",
      "Epoch 08 — MSE train: 15.7869\n",
      "Epoch 09 — MSE train: 15.5369\n",
      "Epoch 10 — MSE train: 15.2715\n",
      "Epoch 11 — MSE train: 14.9870\n",
      "Epoch 12 — MSE train: 14.6791\n",
      "Epoch 13 — MSE train: 14.3425\n",
      "Epoch 14 — MSE train: 13.9706\n",
      "Epoch 15 — MSE train: 13.5556\n",
      "Epoch 16 — MSE train: 13.0880\n",
      "Epoch 17 — MSE train: 12.5573\n",
      "Epoch 18 — MSE train: 11.9518\n",
      "Epoch 19 — MSE train: 11.2600\n",
      "Epoch 20 — MSE train: 10.4727\n",
      "Epoch 21 — MSE train: 9.5874\n",
      "Epoch 22 — MSE train: 8.6155\n",
      "Epoch 23 — MSE train: 7.5891\n",
      "Epoch 24 — MSE train: 6.5652\n",
      "Epoch 25 — MSE train: 5.6150\n",
      "Epoch 26 — MSE train: 4.8019\n",
      "Epoch 27 — MSE train: 4.1619\n",
      "Epoch 28 — MSE train: 3.6939\n",
      "Epoch 29 — MSE train: 3.3692\n",
      "Epoch 30 — MSE train: 3.1540\n",
      "\n",
      "RMSE RNN (LSTM): 1.7910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Usuario-por-película y soportes\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u) / total_users for m, u in users_by_movie.items()}\n",
    "\n",
    "# 3. Generar ítems frecuentes de tamaño 2 y 3 (soporte ≥ 0.2)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {frozenset({m1, m2}): len(users_by_movie[m1] & users_by_movie[m2]) / total_users\n",
    "                  for m1, m2 in combinations(users_by_movie.keys(), 2)\n",
    "                  if len(users_by_movie[m1] & users_by_movie[m2]) / total_users >= min_support}\n",
    "frequent_triples = {frozenset({m1, m2, m3}):\n",
    "                    len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users\n",
    "                    for m1, m2, m3 in combinations(users_by_movie.keys(), 3)\n",
    "                    if len(users_by_movie[m1] & users_by_movie[m2] & users_by_movie[m3]) / total_users >= min_support}\n",
    "\n",
    "# 4. Extracción de features avanzadas\n",
    "def apriori_features_ultimate(row):\n",
    "    user = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated = set(df[df['userId'] == user]['movieId']) - {target}\n",
    "    \n",
    "    # Unarios\n",
    "    sup_target = movie_support.get(target, 0.0)\n",
    "    cnt_rated = len(rated)\n",
    "    \n",
    "    # Inicializar\n",
    "    pair_supports = []\n",
    "    pair_leverages = []\n",
    "    confs = []\n",
    "    lifts = []\n",
    "    weighted_ratings = []\n",
    "    \n",
    "    triple_supports = []\n",
    "    triple_leverages = []\n",
    "    triple_lifts = []\n",
    "    \n",
    "    for other in rated:\n",
    "        pair = frozenset({target, other})\n",
    "        if pair in frequent_pairs:\n",
    "            sup = frequent_pairs[pair]\n",
    "            pair_supports.append(sup)\n",
    "            t_sup = sup_target\n",
    "            o_sup = movie_support.get(other, 0)\n",
    "            pair_leverages.append(sup - t_sup * o_sup)\n",
    "            if t_sup > 0:\n",
    "                confs.append(sup / t_sup)\n",
    "            if t_sup > 0 and o_sup > 0:\n",
    "                lifts.append(sup / (t_sup * o_sup))\n",
    "            # Weighted by support\n",
    "            rating_other = df[(df['userId'] == user) & (df['movieId'] == other)]['rating'].iloc[0]\n",
    "            weighted_ratings.append(sup * rating_other)\n",
    "    \n",
    "    for combo in combinations(rated, 2):\n",
    "        triple = frozenset((target,) + combo)\n",
    "        if triple in frequent_triples:\n",
    "            sup3 = frequent_triples[triple]\n",
    "            triple_supports.append(sup3)\n",
    "            t_sup = sup_target\n",
    "            o1_sup = movie_support.get(combo[0], 0)\n",
    "            o2_sup = movie_support.get(combo[1], 0)\n",
    "            triple_leverages.append(sup3 - t_sup * o1_sup * o2_sup)\n",
    "            if t_sup > 0 and o1_sup > 0 and o2_sup > 0:\n",
    "                triple_lifts.append(sup3 / (t_sup * o1_sup * o2_sup))\n",
    "    \n",
    "    # Features cálculo\n",
    "    return pd.Series({\n",
    "        'sup_target': sup_target,\n",
    "        'cnt_rated': cnt_rated,\n",
    "        'freq_pair_count': len(pair_supports),\n",
    "        'freq_pair_support_sum': sum(pair_supports),\n",
    "        'max_pair_support': max(pair_supports) if pair_supports else 0.0,\n",
    "        'min_pair_support': min(pair_supports) if pair_supports else 0.0,\n",
    "        'avg_pair_support': sum(pair_supports)/len(pair_supports) if pair_supports else 0.0,\n",
    "        'sum_pair_leverage': sum(pair_leverages),\n",
    "        'max_pair_leverage': max(pair_leverages) if pair_leverages else 0.0,\n",
    "        'max_pair_confidence': max(confs) if confs else 0.0,\n",
    "        'avg_pair_lift': sum(lifts)/len(lifts) if lifts else 0.0,\n",
    "        'max_pair_lift': max(lifts) if lifts else 0.0,\n",
    "        'weighted_avg_rating_pair': sum(weighted_ratings)/sum(pair_supports) if pair_supports else 0.0,\n",
    "        'freq_triple_count': len(triple_supports),\n",
    "        'freq_triple_support_sum': sum(triple_supports),\n",
    "        'avg_triple_support': sum(triple_supports)/len(triple_supports) if triple_supports else 0.0,\n",
    "        'max_triple_support': max(triple_supports) if triple_supports else 0.0,\n",
    "        'sum_triple_leverage': sum(triple_leverages),\n",
    "        'max_triple_lift': max(triple_lifts) if triple_lifts else 0.0,\n",
    "        'avg_triple_lift': sum(triple_lifts)/len(triple_lifts) if triple_lifts else 0.0,\n",
    "        'triple_coverage': len(triple_supports)/ (cnt_rated*(cnt_rated-1)/2) if cnt_rated > 1 else 0.0\n",
    "    })\n",
    "\n",
    "# 5. Generar dataset final\n",
    "ultimate_feats = df.apply(apriori_features_ultimate, axis=1)\n",
    "df_ult = pd.concat([df, ultimate_feats], axis=1).drop(['userId','movieId','timestamp'], axis=1)\n",
    "X = df_ult.drop('rating', axis=1)\n",
    "y = df_ult['rating']\n",
    "\n",
    "# 6. Dividir y entrenar modelos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 1) Convertir DataFrames a numpy float32\n",
    "X_train_np = X_train.to_numpy(dtype=np.float32)\n",
    "y_train_np = y_train.to_numpy(dtype=np.float32)\n",
    "X_test_np  = X_test.to_numpy(dtype=np.float32)\n",
    "y_test_np  = y_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "# 2) Crear tensores y añadir dimensión de “feature”\n",
    "#    De (N, seq_len) a (N, seq_len, 1)\n",
    "X_train_t = torch.from_numpy(X_train_np).unsqueeze(2)\n",
    "y_train_t = torch.from_numpy(y_train_np)\n",
    "X_test_t  = torch.from_numpy(X_test_np).unsqueeze(2)\n",
    "y_test_t  = torch.from_numpy(y_test_np)\n",
    "\n",
    "# 3) DataLoader\n",
    "batch_size = 512\n",
    "train_ds   = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 4) Definir el modelo RNN (LSTM + lineal)\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size=64, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 1)\n",
    "        out, _ = self.lstm(x)               # (batch, seq_len, hidden_size)\n",
    "        last = out[:, -1, :]                # tomar salida del último paso\n",
    "        return self.fc(last).squeeze(1)     # (batch,)\n",
    "\n",
    "# 5) Preparar dispositivo y modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seq_len = X_train_t.shape[1]\n",
    "model   = RNNRegressor(seq_len=seq_len, hidden_size=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 6) Entrenamiento\n",
    "n_epochs = 30\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    mse_train = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:02d} — MSE train: {mse_train:.4f}\")\n",
    "\n",
    "# 7) Evaluación en test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(X_test_t.to(device)).cpu().numpy()\n",
    "rmse_rnn = sqrt(mean_squared_error(y_test_np, preds_test))\n",
    "print(f\"\\nRMSE RNN (LSTM): {rmse_rnn:.4f}\")\n",
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54befd23",
   "metadata": {},
   "source": [
    "## NN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7494bc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Neural Network: 1.6304\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv('ratings2comoML.csv')\n",
    "\n",
    "# 2. Usuario-por-película y soportes\n",
    "total_users = df['userId'].nunique()\n",
    "users_by_movie = df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support   = {m: len(u)/total_users for m,u in users_by_movie.items()}\n",
    "\n",
    "# 3. Ítems frecuentes (pares y tríos)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset([m1,m2]): len(users_by_movie[m1]&users_by_movie[m2]) / total_users\n",
    "    for m1,m2 in combinations(users_by_movie,2)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1,m2,m3]):\n",
    "      len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users\n",
    "    for m1,m2,m3 in combinations(users_by_movie,3)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Función de extracción de las 21 features Apriori avanzadas\n",
    "def apriori_features_ultimate(row):\n",
    "    user   = row['userId']\n",
    "    target = row['movieId']\n",
    "    rated  = set(df[df['userId']==user]['movieId']) - {target}\n",
    "\n",
    "    sup_t = movie_support.get(target, 0.0)\n",
    "    pc = ps = tc = ts = 0\n",
    "\n",
    "    for other in rated:\n",
    "        p = frozenset([target, other])\n",
    "        if p in frequent_pairs:\n",
    "            s = frequent_pairs[p]\n",
    "            pc += 1; ps += s\n",
    "\n",
    "    for combo in combinations(rated,2):\n",
    "        t = frozenset([target,*combo])\n",
    "        if t in frequent_triples:\n",
    "            s3 = frequent_triples[t]\n",
    "            tc += 1; ts += s3\n",
    "\n",
    "    return pd.Series({\n",
    "        'sup_target': sup_t,\n",
    "        'cnt_rated': len(rated),\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts,\n",
    "        'max_pair_support': max((frequent_pairs[frozenset([target,o])] \n",
    "                                 for o in rated if frozenset([target,o]) in frequent_pairs), default=0.0),\n",
    "        'avg_pair_support': (ps/pc) if pc>0 else 0.0,\n",
    "        'sum_pair_leverage': sum((frequent_pairs[frozenset([target,o])] -\n",
    "                                  sup_t * movie_support[o])\n",
    "                                 for o in rated if frozenset([target,o]) in frequent_pairs),\n",
    "        'max_pair_confidence': max(((frequent_pairs[frozenset([target,o])] / sup_t)\n",
    "                                    for o in rated if sup_t>0 and frozenset([target,o]) in frequent_pairs),\n",
    "                                   default=0.0),\n",
    "        'avg_pair_lift': sum((frequent_pairs[frozenset([target,o])] /\n",
    "                              (sup_t*movie_support[o]))\n",
    "                             for o in rated if sup_t>0 and movie_support[o]>0 and frozenset([target,o]) in frequent_pairs)\n",
    "                          / max(pc,1),\n",
    "        'freq_triple_support_sum': ts,\n",
    "        'triple_coverage': tc / max((len(rated)*(len(rated)-1)/2),1)\n",
    "    })\n",
    "\n",
    "# 5. Generar dataset de features\n",
    "ult_feats = df.apply(apriori_features_ultimate, axis=1)\n",
    "df_ml = pd.concat([df, ult_feats], axis=1).drop(['userId','movieId','timestamp'], axis=1)\n",
    "\n",
    "X = df_ml.drop('rating', axis=1)\n",
    "y = df_ml['rating']\n",
    "\n",
    "# 6. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Pipeline: escalado + MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp',    MLPRegressor(\n",
    "                   hidden_layer_sizes=(100,),\n",
    "                   activation='relu',\n",
    "                   solver='adam',\n",
    "                   max_iter=200,\n",
    "                   random_state=42\n",
    "               ))\n",
    "])\n",
    "\n",
    "# 8. Entrenamiento y evaluación\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "rmse_nn = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"RMSE Neural Network: {rmse_nn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e66f4",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5bed6003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 18.2143 \n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 17.7385\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 16.8936\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 16.8628\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 16.6916\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 15.8470\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 16.2378\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 15.7300\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 14.6093\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.6057\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D49B3C6A20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "RMSE RNN: 3.6133\n",
      "Numero de features: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# 1. Carga de datos\n",
    "df_ratings = pd.read_csv('ratings2comoML.csv')\n",
    "df_movies  = pd.read_csv('movies.csv', usecols=['movieId','genres'])\n",
    "movie_genres = df_movies.set_index('movieId')['genres'].str.split('|').to_dict()\n",
    "\n",
    "# 2. Pre-cálculo de soportes\n",
    "total_users = df_ratings['userId'].nunique()\n",
    "users_by_movie = df_ratings.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "movie_support = {m: len(u)/total_users for m,u in users_by_movie.items()}\n",
    "\n",
    "# 3. Ítems frecuentes (pares y tríos)\n",
    "min_support = 0.2\n",
    "frequent_pairs = {\n",
    "    frozenset([m1,m2]): len(users_by_movie[m1]&users_by_movie[m2]) / total_users\n",
    "    for m1,m2 in combinations(users_by_movie,2)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]) / total_users >= min_support\n",
    "}\n",
    "frequent_triples = {\n",
    "    frozenset([m1,m2,m3]):\n",
    "      len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users\n",
    "    for m1,m2,m3 in combinations(users_by_movie,3)\n",
    "    if len(users_by_movie[m1]&users_by_movie[m2]&users_by_movie[m3]) / total_users >= min_support\n",
    "}\n",
    "\n",
    "# 4. Extracción de las 10 features\n",
    "def extract_features(row):\n",
    "    user, target = row['userId'], row['movieId']\n",
    "    rated = set(df_ratings[df_ratings['userId']==user]['movieId']) - {target}\n",
    "    sup_t = movie_support.get(target,0.0)\n",
    "    cnt_r = len(rated)\n",
    "    pc = ps = tc = ts = 0\n",
    "    for other in rated:\n",
    "        p = frozenset([target,other])\n",
    "        if p in frequent_pairs:\n",
    "            s = frequent_pairs[p]\n",
    "            pc += 1; ps += s\n",
    "    for combo in combinations(rated,2):\n",
    "        t = frozenset([target,*combo])\n",
    "        if t in frequent_triples:\n",
    "            s3 = frequent_triples[t]\n",
    "            tc += 1; ts += s3\n",
    "    feats = {\n",
    "        'sup_target': sup_t,\n",
    "        'cnt_rated': cnt_r,\n",
    "        'freq_pair_count': pc,\n",
    "        'freq_pair_support_sum': ps,\n",
    "        'freq_triple_count': tc,\n",
    "        'freq_triple_support_sum': ts,\n",
    "    }\n",
    "    genres = movie_genres.get(target, [])\n",
    "    feats['num_genres']   = len(genres)\n",
    "    feats['is_Comedy']    = int('Comedy' in genres)\n",
    "    feats['is_Drama']     = int('Drama' in genres)\n",
    "    feats['is_Thriller']  = int('Thriller' in genres)\n",
    "    return pd.Series(feats)\n",
    "\n",
    "feature_df = df_ratings.apply(extract_features, axis=1)\n",
    "X = feature_df.values\n",
    "y = df_ratings['rating'].values\n",
    "\n",
    "# 5. Escalado y reshape para RNN (timesteps=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_seq = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# 6. Train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Definir y entrenar la RNN\n",
    "n_features = X_seq.shape[2]\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, activation='relu', input_shape=(1, n_features)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Ajusta epochs/batch_size según tu máquina\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# 8. Evaluación\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "rmse_rnn = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE RNN: {rmse_rnn:.4f}\")\n",
    "\n",
    "# 9. Número de features\n",
    "print(f\"Numero de features: {X.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
